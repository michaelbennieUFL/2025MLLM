# Towards Human Cognition: Visual Context Guides Syntactic Priming in Fusion-Encoded Models

This repository contains the implementation of our fusion-encoded model described in the paper **"Towards Human Cognition: Visual Context Guides Syntactic Priming in Fusion-Encoded Models."** In this work, we propose a novel multimodal approach that leverages visual context to guide syntactic priming in language generation. Our model fuses visual and textual embeddings to condition a pretrained LLaMA language model via soft prompt tokens, thereby emulating human-like syntactic priming effects.

---

## Table of Contents

- [Overview](#overview)
- [Key Components](#key-components)
- [Installation](#installation)
- [Usage](#usage)
- [Dataset](#dataset)
- [Contact](#contact)
- [License](#license)

---

## Overview

Multimodal language models have demonstrated remarkable capabilities in integrating visual and linguistic information. In our paper, we introduce a fusion-encoded architecture that:

- **Fuses two image embeddings and one text embedding** using a multilayer perceptron (MLP) to generate soft prompt tokens.
- **Preconditions a frozen LLaMA decoder** by prepending these soft tokens to the textual input.
- **Evaluates syntactic priming effects** via a novel metric based on tree kernel similarity.

This implementation corresponds to our **Model 1** (the fusion-encoded variant) and serves as a building block for studying how visual context can guide syntactic choices in generated language.

---

## Key Components

- **Fusion Module:**  
  Implements an MLP that fuses image and text embeddings and projects the result into a sequence of soft prompt tokens.  
  *Location:* `src/simpleMLPfusion/fusion_module.py`
  
- **LLaMA Decoder with Prefix:**  
  Wraps a frozen LLaMA language model to accept the soft prompt tokens as a prefix, enabling controlled text generation.  
  *Location:* `src/simpleMLPfusion/decoder_with_prefix.py`
  
- **Dataset Handler:**  
  Provides routines to load and preprocess the TSV dataset containing precomputed image and text embeddings along with candidate captions.  
  *Location:* `src/simpleMLPfusion/dataset_handler.py`
  
- **Training and Evaluation Script:**  
  Integrates the fusion module and LLaMA decoder, trains the model with checkpointing, and evaluates syntactic priming using our proposed metric.  
  *Location:* `src/testMLPModel.py`
  


---

## Installation

### Prerequisites

- Python ≥ 3.8
- [PyTorch](https://pytorch.org/) ≥ 1.10
- [Transformers](https://github.com/huggingface/transformers)


---

## Usage

### Training & Evaluation

To train the fusion-encoded model and evaluate its syntactic priming performance, simply run:

```
python src/testMLPModel.py
```

This script performs the following:

- Loads the preprocessed TSV dataset.
- Initializes the fusion module and LLaMA decoder.
- Trains the model with checkpointing.
- Periodically evaluates the model using our novel priming effect (PE) metric.

### Inference

After training, the script will generate example captions conditioned on given image and text inputs, demonstrating the effect of the priming mechanism.

---

## Dataset

The repository expects the data to be organized as follows:

```
data/
├── coco_ann2017/annotations/
├── preprocessed/embeddings_prismatic.csv
└── test_set/
    ├── data.csv
    └── selected_images/
```

- **COCO Annotations:** Contains JSON files for captions and instances.
- **Preprocessed Data:** CSV file with image and text embeddings generated by our preprocessor.
- **Test Set:** A CSV file along with a directory of selected images for evaluation.

Ensure you have the necessary data files in the correct locations or update the file paths in the source code accordingly.


---

## Contact

For any questions or feedback, please contact:

BuShi Xiao – xiaobushi@ufl.edu

Michael Bennie - michaelbennie@ufl.edu

---

## License

This project is licensed under the GPL-3.0 license. See the LICENSE file for details.
